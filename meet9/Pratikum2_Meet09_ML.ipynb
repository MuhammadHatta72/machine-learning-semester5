{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1eimJ2Rxq_FuPV84jk4650JO2db8LenJ4",
      "authorship_tag": "ABX9TyMASKyhO+YP1zzZN3oIGcm3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MuhammadHatta72/machine-learning-semester5/blob/main/meet9/Pratikum2_Meet09_ML.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Nama : Muhammad Hatta\n",
        "###Kelas : 3A-TI\n",
        "###Mata Kuliah : Machine Learning"
      ],
      "metadata": {
        "id": "G1vSqj0bOncb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import data\n",
        "from sklearn.datasets import fetch_20newsgroups # download dataset\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import Perceptron\n",
        "from sklearn.metrics import f1_score, classification_report"
      ],
      "metadata": {
        "id": "ZkX6mECtT_q3"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pilih label dan split data\n",
        "categories = ['rec.sport.hockey', 'rec.sport.baseball', 'rec.autos']\n",
        "newsgroups_train = fetch_20newsgroups(subset='train', categories=categories, remove=('headers', 'footers', 'quotes'))\n",
        "newsgroups_test = fetch_20newsgroups(subset='test', categories=categories, remove=('headers', 'footers', 'quotes'))"
      ],
      "metadata": {
        "id": "5sNr41VAUDG2"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ekstrak Fitur dan Buat Model Perceptron\n",
        "# Ekstrak Fitur\n",
        "vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Fit fitur\n",
        "X_train = vectorizer.fit_transform(newsgroups_train.data)\n",
        "X_test = vectorizer.transform(newsgroups_test.data)\n",
        "\n",
        "# Fit Model\n",
        "clf = Perceptron(random_state=11)\n",
        "clf.fit(X_train, newsgroups_train.target)\n",
        "\n",
        "# Prediksi\n",
        "predictions = clf.predict(X_test)\n",
        "print(classification_report(newsgroups_test.target, predictions))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0IrKAwkZUHXJ",
        "outputId": "5f838427-2e68-47e9-e4d9-6c11c47ba71a"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.88      0.88      0.88       396\n",
            "           1       0.82      0.83      0.83       397\n",
            "           2       0.88      0.87      0.87       399\n",
            "\n",
            "    accuracy                           0.86      1192\n",
            "   macro avg       0.86      0.86      0.86      1192\n",
            "weighted avg       0.86      0.86      0.86      1192\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Penjelasan\n",
        "Dataset yang digunakan pada kode program diatas adalah 20newsgroup yang terdiri dari sekitar 20.000 dokumen. Scikit-learn bahkan menyediakan fungsi yang memberikan kemudahan untuk mengunduh dan membaca kumpulan dataset dengan menggunakan sklearn.datasets. pada kode program diatas Perceptron mampu melakukan klasifikasi multikelas; strategi yang digunakan adalah one-versus-all untuk melakukan pelatihan untuk setiap kelas dalam data training. Dokumen teks memerlukan ekstraksi fitur salah satunya adalah bobot tf-idf pada kodeprogram diatas digunakan tfidf-vectorizer."
      ],
      "metadata": {
        "id": "wKDY6ygRUUU-"
      }
    }
  ]
}