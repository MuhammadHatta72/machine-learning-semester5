{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPM5WYYuU911sv/2bj8ueIz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MuhammadHatta72/machine-learning-semester5/blob/main/meet10/Pratikum2_Meet10_ML.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Nama : Muhammad Hatta\n",
        "##Kelas : 3A-TI\n",
        "##Mata Kuliah : Machine Learning"
      ],
      "metadata": {
        "id": "X3kmTrpXJLIF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import TensorFlow\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "import time"
      ],
      "metadata": {
        "id": "i2WNzdujIwLj"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download Dataset Shakespeare\n",
        "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uj7ywO3AI6CI",
        "outputId": "bcec3a1c-2a6f-4b00-a94f-e06de5e3cd83"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
            "1115394/1115394 [==============================] - 1s 1us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Data\n",
        "# Read, then decode for py2 compat.\n",
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
        "# length of text is the number of characters in it\n",
        "print(f'Length of text: {len(text)} characters')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U2U-itL2JQCu",
        "outputId": "01f9e081-47d6-4c14-f02f-bcc624f4e95c"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of text: 1115394 characters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Take a look at the first 250 characters in text\n",
        "print(text[:250])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i0jbxX7dJXJA",
        "outputId": "11996ce5-6092-447b-91be-ae20d7666af2"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The unique characters in the file\n",
        "vocab = sorted(set(text))\n",
        "print(f'{len(vocab)} unique characters')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ntG3SO0AJZbl",
        "outputId": "6cde779b-e3c8-413b-ec81-e321197f57d1"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "65 unique characters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Olah Teks\n",
        "# Vectorize Teks\n",
        "example_texts = ['abcdefg', 'xyz']\n",
        "chars = tf.strings.unicode_split(example_texts, input_encoding='UTF-8')\n",
        "chars"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VoQKs-kpJa_m",
        "outputId": "c713634b-e9a7-46f0-930b-6fb531d48973"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# sekarang buat tf.keras.layers.StringLookup layer:\n",
        "ids_from_chars = tf.keras.layers.StringLookup(vocabulary=list(vocab), mask_token=None)"
      ],
      "metadata": {
        "id": "r0f1kQA8JnDS"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# perintah diatas mengconvert token menjadi id\n",
        "ids = ids_from_chars(chars)\n",
        "ids"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TeubiVzZJwAe",
        "outputId": "a8f41627-16e8-4c2a-8a32-dd6fac24a208"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[40, 41, 42, 43, 44, 45, 46], [63, 64, 65]]>"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# kosakata asli yang dihasilkan dengan diurutkan(set(teks)) gunakan metode get_vocabulary()\n",
        "chars_from_ids = tf.keras.layers.StringLookup(\n",
        "    vocabulary=ids_from_chars.get_vocabulary(), invert=True, mask_token=None)"
      ],
      "metadata": {
        "id": "wTqPQsYZJ2fa"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Lapisan ini mengconvert kembali karakter dari vektor ID, dan mengembalikannya sebagai karakter tf.RaggedTensor\n",
        "chars = chars_from_ids(ids)\n",
        "chars"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uqhSjEADJ-Np",
        "outputId": "79e5ee32-b22a-4600-abc4-3d3afde6baac"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Anda dapat menggunakan tf.strings.reduce_join untuk menggabungkan kembali karakter menjadi string.\n",
        "tf.strings.reduce_join(chars, axis=-1).numpy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UM-nVHEMKEzp",
        "outputId": "3ec1ff7c-c6df-4d99-cc07-0003c4d3cd7c"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([b'abcdefg', b'xyz'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def text_from_ids(ids):\n",
        "    return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)"
      ],
      "metadata": {
        "id": "I0e5SeVVKRfo"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prediksi\n",
        "# mengonversi vektor teks menjadi aliran indeks karakter.\n",
        "all_ids = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8'))\n",
        "all_ids"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fqTtRvgsKX0u",
        "outputId": "a5851569-21ad-4790-b8b8-e6108a5b00d5"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1115394,), dtype=int64, numpy=array([19, 48, 57, ..., 46,  9,  1])>"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)"
      ],
      "metadata": {
        "id": "UE46kR_3Kn1I"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for ids in ids_dataset.take(10):\n",
        "    print(chars_from_ids(ids).numpy().decode('utf-8'))\n",
        "\n",
        "seq_length = 100"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0JiqA6JtKric",
        "outputId": "f646fa96-0ffe-4088-8e27-9b25fd510224"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F\n",
            "i\n",
            "r\n",
            "s\n",
            "t\n",
            " \n",
            "C\n",
            "i\n",
            "t\n",
            "i\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Metode batch memungkinkan Anda dengan mudah mengonversi karakter individual ini menjadi urutan ukuran yang diinginkan.\n",
        "sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "\n",
        "for seq in sequences.take(1):\n",
        "  print(chars_from_ids(seq))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MC1xJlO4KzDo",
        "outputId": "b026227f-735b-4ac6-ecff-14ead404d8a4"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[b'F' b'i' b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':'\n",
            " b'\\n' b'B' b'e' b'f' b'o' b'r' b'e' b' ' b'w' b'e' b' ' b'p' b'r' b'o'\n",
            " b'c' b'e' b'e' b'd' b' ' b'a' b'n' b'y' b' ' b'f' b'u' b'r' b't' b'h'\n",
            " b'e' b'r' b',' b' ' b'h' b'e' b'a' b'r' b' ' b'm' b'e' b' ' b's' b'p'\n",
            " b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'A' b'l' b'l' b':' b'\\n' b'S' b'p' b'e'\n",
            " b'a' b'k' b',' b' ' b's' b'p' b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'F' b'i'\n",
            " b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':' b'\\n' b'Y'\n",
            " b'o' b'u' b' '], shape=(101,), dtype=string)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# akan lebih mudah untuk melihat apa yang dilakukan jika Anda menggabungkan token kembali menjadi string:\n",
        "for seq in sequences.take(5):\n",
        "    print(text_from_ids(seq).numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QFv7uYcSK3_C",
        "outputId": "9b96e001-c995-4992-e891-16e43dedd21d"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n",
            "b'are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you k'\n",
            "b\"now Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us ki\"\n",
            "b\"ll him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be d\"\n",
            "b'one: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citi'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# fungsi yang mengambil urutan sebagai masukan, menduplikasi, dan menggesernya untuk menyelaraskan masukan dan label untuk setiap langkah waktu\n",
        "def split_input_target(sequence):\n",
        "  input_text = sequence[:-1]\n",
        "  target_text = sequence[1:]\n",
        "  return input_text, target_text\n",
        "\n",
        "split_input_target(list(\"Tensorflow\"))\n",
        "dataset = sequences.map(split_input_target)"
      ],
      "metadata": {
        "id": "Ndbwkp0iK8vg"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for input_example, target_example in dataset.take(1):\n",
        "  print(\"Input :\", text_from_ids(input_example).numpy())\n",
        "  print(\"Target:\", text_from_ids(target_example).numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nKM4eWg6LOrg",
        "outputId": "5d7ee00e-fdbf-483d-e2fd-395eac2bb077"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input : b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'\n",
            "Target: b'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Membuat Batch Training\n",
        "# Batch size\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# Buffer size to shuffle the dataset\n",
        "# (TF data is designed to work with possibly infinite sequences,\n",
        "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
        "# it maintains a buffer in which it shuffles elements).\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = (\n",
        "    dataset\n",
        "    .shuffle(BUFFER_SIZE)\n",
        "    .batch(BATCH_SIZE, drop_remainder=True)\n",
        "    .prefetch(tf.data.experimental.AUTOTUNE))\n",
        "\n",
        "dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OugjkBk7LWaB",
        "outputId": "9b736036-3b59-4919-86e7-9f695df3eacb"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<_PrefetchDataset element_spec=(TensorSpec(shape=(64, 100), dtype=tf.int64, name=None), TensorSpec(shape=(64, 100), dtype=tf.int64, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Buat Model\n",
        "# Length of the vocabulary in StringLookup Layer\n",
        "vocab_size = len(ids_from_chars.get_vocabulary())\n",
        "\n",
        "# The embedding dimension\n",
        "embedding_dim = 256\n",
        "\n",
        "# Number of RNN units\n",
        "rnn_units = 1024"
      ],
      "metadata": {
        "id": "qyp0gNLjLb1f"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MyModel(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
        "    super().__init__(self)\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(rnn_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True)\n",
        "    self.dense = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "  def call(self, inputs, states=None, return_state=False, training=False):\n",
        "    x = inputs\n",
        "    x = self.embedding(x, training=training)\n",
        "    if states is None:\n",
        "      states = self.gru.get_initial_state(x)\n",
        "    x, states = self.gru(x, initial_state=states, training=training)\n",
        "    x = self.dense(x, training=training)\n",
        "\n",
        "    if return_state:\n",
        "      return x, states\n",
        "    else:\n",
        "      return x"
      ],
      "metadata": {
        "id": "b6stBk5gLhJj"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = MyModel(\n",
        "    vocab_size=vocab_size,\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units)"
      ],
      "metadata": {
        "id": "XFpbnBNKLjXp"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Uji Model\n",
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "    example_batch_predictions = model(input_example_batch)\n",
        "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8FgszJmWLlgz",
        "outputId": "82f4f524-93dd-42d7-f97e-b8f46dc36b60"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 100, 66) # (batch_size, sequence_length, vocab_size)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RiBZywFFLpbI",
        "outputId": "ac7ec97f-3c57-4779-e0d4-97f888c5c90d"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"my_model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_2 (Embedding)     multiple                  16896     \n",
            "                                                                 \n",
            " gru (GRU)                   multiple                  3938304   \n",
            "                                                                 \n",
            " dense_4 (Dense)             multiple                  67650     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4022850 (15.35 MB)\n",
            "Trainable params: 4022850 (15.35 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
        "sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()"
      ],
      "metadata": {
        "id": "10WQVuj8LsLK"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sampled_indices"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "coJkQkWFLunh",
        "outputId": "85c68cf3-6862-42e6-daa6-ab9bd2e94cb3"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([18, 25, 62, 54, 14, 55, 64, 56, 58,  0, 42,  4, 21, 34, 61,  3, 63,\n",
              "       24, 19, 57, 30, 53, 10, 46, 29, 58, 50, 38, 30, 20, 62, 62, 47, 36,\n",
              "       56, 54, 12, 50, 65, 39,  0,  7, 13, 51, 53,  2, 44, 57, 62, 51, 21,\n",
              "       34, 58, 27,  1, 20, 11, 30, 23,  6, 27, 61, 46, 11,  0, 48, 44, 23,\n",
              "       27, 14, 23, 35, 44, 22, 11, 53,  3, 23, 46, 47,  4,  8, 47, 38, 44,\n",
              "       52, 46, 28, 30, 27,  1, 61, 40, 50, 60, 31, 31, 43, 25, 62])"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Input:\\n\", text_from_ids(input_example_batch[0]).numpy())\n",
        "print()\n",
        "print(\"Next Char Predictions:\\n\", text_from_ids(sampled_indices).numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zdCKRaj1L0g-",
        "outputId": "6b7ac377-c052-4264-b909-849f212dd037"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input:\n",
            " b\"olong awhile the traitor's life.\\nWrath makes him deaf: speak thou, Northumberland.\\n\\nNORTHUMBERLAND:\\n\"\n",
            "\n",
            "Next Char Predictions:\n",
            " b\"ELwoApyqs[UNK]c$HUv!xKFrQn3gPskYQGwwhWqo;kzZ[UNK],?ln erwlHUsN\\nG:QJ'Nvg:[UNK]ieJNAJVeI:n!Jgh$-hYemgOQN\\nvakuRRdLw\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)"
      ],
      "metadata": {
        "id": "7l-7R8rYL4rE"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example_batch_mean_loss = loss(target_example_batch, example_batch_predictions)\n",
        "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
        "print(\"Mean loss:        \", example_batch_mean_loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PgzZBzMjL8Vw",
        "outputId": "ba2b3504-71dc-4a4f-ca71-82271029e732"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction shape:  (64, 100, 66)  # (batch_size, sequence_length, vocab_size)\n",
            "Mean loss:         tf.Tensor(4.18938, shape=(), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.exp(example_batch_mean_loss).numpy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qKlnNaMOL_C1",
        "outputId": "862d7795-86eb-4493-e869-267bc3f77981"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "65.98188"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam',loss=loss)"
      ],
      "metadata": {
        "id": "UivwcqLTMCHg"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Konfigurasi Checkpoints\n",
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)"
      ],
      "metadata": {
        "id": "RXUu7UoSME81"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Lakukan Proses Training\n",
        "EPOCHS = 20\n",
        "\n",
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "377zZLuMMJwr",
        "outputId": "aac069fa-48d0-4abd-aa65-42b1d3aa2516"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "172/172 [==============================] - 19s 54ms/step - loss: 2.7166\n",
            "Epoch 2/20\n",
            "172/172 [==============================] - 11s 55ms/step - loss: 1.9901\n",
            "Epoch 3/20\n",
            "172/172 [==============================] - 12s 60ms/step - loss: 1.7090\n",
            "Epoch 4/20\n",
            "172/172 [==============================] - 12s 53ms/step - loss: 1.5451\n",
            "Epoch 5/20\n",
            "172/172 [==============================] - 12s 54ms/step - loss: 1.4451\n",
            "Epoch 6/20\n",
            "172/172 [==============================] - 12s 53ms/step - loss: 1.3772\n",
            "Epoch 7/20\n",
            "172/172 [==============================] - 12s 54ms/step - loss: 1.3247\n",
            "Epoch 8/20\n",
            "172/172 [==============================] - 11s 54ms/step - loss: 1.2777\n",
            "Epoch 9/20\n",
            "172/172 [==============================] - 12s 57ms/step - loss: 1.2370\n",
            "Epoch 10/20\n",
            "172/172 [==============================] - 11s 57ms/step - loss: 1.1976\n",
            "Epoch 11/20\n",
            "172/172 [==============================] - 12s 58ms/step - loss: 1.1565\n",
            "Epoch 12/20\n",
            "172/172 [==============================] - 12s 59ms/step - loss: 1.1158\n",
            "Epoch 13/20\n",
            "172/172 [==============================] - 11s 56ms/step - loss: 1.0711\n",
            "Epoch 14/20\n",
            "172/172 [==============================] - 11s 54ms/step - loss: 1.0245\n",
            "Epoch 15/20\n",
            "172/172 [==============================] - 11s 55ms/step - loss: 0.9761\n",
            "Epoch 16/20\n",
            "172/172 [==============================] - 11s 55ms/step - loss: 0.9241\n",
            "Epoch 17/20\n",
            "172/172 [==============================] - 11s 55ms/step - loss: 0.8711\n",
            "Epoch 18/20\n",
            "172/172 [==============================] - 11s 56ms/step - loss: 0.8196\n",
            "Epoch 19/20\n",
            "172/172 [==============================] - 11s 55ms/step - loss: 0.7682\n",
            "Epoch 20/20\n",
            "172/172 [==============================] - 11s 55ms/step - loss: 0.7192\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Berikut ini membuat prediksi satu langkah\n",
        "class OneStep(tf.keras.Model):\n",
        "  def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
        "    super().__init__()\n",
        "    self.temperature = temperature\n",
        "    self.model = model\n",
        "    self.chars_from_ids = chars_from_ids\n",
        "    self.ids_from_chars = ids_from_chars\n",
        "\n",
        "    # Create a mask to prevent \"[UNK]\" from being generated.\n",
        "    skip_ids = self.ids_from_chars(['[UNK]'])[:, None]\n",
        "    sparse_mask = tf.SparseTensor(\n",
        "        # Put a -inf at each bad index.\n",
        "        values=[-float('inf')]*len(skip_ids),\n",
        "        indices=skip_ids,\n",
        "        # Match the shape to the vocabulary\n",
        "        dense_shape=[len(ids_from_chars.get_vocabulary())])\n",
        "    self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
        "\n",
        "  @tf.function\n",
        "  def generate_one_step(self, inputs, states=None):\n",
        "    # Convert strings to token IDs.\n",
        "    input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
        "    input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
        "\n",
        "    # Run the model.\n",
        "    # predicted_logits.shape is [batch, char, next_char_logits]\n",
        "    predicted_logits, states = self.model(inputs=input_ids, states=states,\n",
        "                                          return_state=True)\n",
        "    # Only use the last prediction.\n",
        "    predicted_logits = predicted_logits[:, -1, :]\n",
        "    predicted_logits = predicted_logits/self.temperature\n",
        "    # Apply the prediction mask: prevent \"[UNK]\" from being generated.\n",
        "    predicted_logits = predicted_logits + self.prediction_mask\n",
        "\n",
        "    # Sample the output logits to generate token IDs.\n",
        "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
        "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
        "\n",
        "    # Convert from token ids to characters\n",
        "    predicted_chars = self.chars_from_ids(predicted_ids)\n",
        "\n",
        "    # Return the characters and model state.\n",
        "    return predicted_chars, states"
      ],
      "metadata": {
        "id": "cWCP7LzJMSkE"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "one_step_model = OneStep(model, chars_from_ids, ids_from_chars)"
      ],
      "metadata": {
        "id": "mPgcmQwFMbT-"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Karena sedikitnya jumlah epoch pelatihan, model belum belajar membentuk kalimat runtut.\n",
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant(['ROMEO:'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(1000):\n",
        "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\n",
        "print('\\nRun time:', end - start)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8YX4AtExMehc",
        "outputId": "8d76705b-1369-4ee9-8f8d-02a55a3488f7"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROMEO:\n",
            "Be merry, girl.\n",
            "\n",
            "AUTOLYCUS:\n",
            "\n",
            "Clown:\n",
            "Think you, Well, beat not made the child.\n",
            "\n",
            "CORIOLANUS:\n",
            "Thanks, good-face!\n",
            "Your princess how to earth service, but I am lour\n",
            "a slepperor.\n",
            "\n",
            "BIRANDA:\n",
            "O, good my lord, lest both return.\n",
            "\n",
            "KING RICHARD II:\n",
            "The needy children of our fees,\n",
            "With dignior to be frenchwed when accordion\n",
            "Which now be loss of careing flowers.\n",
            "\n",
            "LADY ANNE:\n",
            "Foul weather, a word, then; where, if Warwick busy\n",
            "The under care, away with flight wept;\n",
            "Romeo, a man,--why, how I remember,\n",
            "Gentleman, the secret nurse whose honour\n",
            "I'll prove more hoats on thee; for perhin mine\n",
            "Against a purse against it AMposition of a stare\n",
            "To some alone: sir, that have been a\n",
            "vice. I have and the dir Inow good.\n",
            "When she is young and head! My business are mine\n",
            "fair vilerbusion; and jors, I fear,--\n",
            "As famous like a little adminers,\n",
            "But farewell king, she was too hot a sin.\n",
            "\n",
            "BUCKINGHAM:\n",
            "Why looks your great delights thine owe\n",
            "Men your majest worm of such as you,\n",
            "That calls our partical; who at the other crack? \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 3.4885571002960205\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# model menghasilkan 5 keluaran dalam waktu yang hampir sama dengan waktu yang dibutuhkan untuk menghasilkan 1 keluaran di atas.\n",
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant(['ROMEO:', 'ROMEO:', 'ROMEO:', 'ROMEO:', 'ROMEO:'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(1000):\n",
        "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "print(result, '\\n\\n' + '_'*80)\n",
        "print('\\nRun time:', end - start)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GAPi6svLMwbJ",
        "outputId": "59d27f79-e825-46e0-f09b-5ef51cb0c25e"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[b\"ROMEO:\\nShe is, it dies, the earlows too.\\n\\nELBOW:\\nMistress Are ill comfort of all died,\\nDefiniting the officers of such sea\\nTo tread oppression of himself then\\nhonest end.\\n\\nMERCUTIO:\\nThe news is virtue. Dear joy I alt all thanks:\\nSome of those shepeties and true honour you\\nand honours shall die by you take it as affers.\\n\\nGLOUCESTER:\\n\\nKING RICHARD II:\\nTarsh and such carcess past thy lips?\\n\\nJULIET:\\nWhat fats is out at Dainty but news toward?\\n\\nLORD WISTETHARD:\\nThen should they be in heavency to this fair,\\nSpeaking clean, and rid o'er run at ancient sun\\nPrepare the broker in obles of his most\\nUntainted clamour of his lips,\\nPity may remember more carries of his.\\n\\nYORK:\\nThou shalt be married to her love.\\n\\nROMEO:\\nO, when you call back before this day.\\n\\nPETER:\\nO so, be gone.\\n\\nCORIOLANUS:\\nNay, then I know my bust shall stay.\\n\\nHENRY BOLINGBROKE:\\nWith all shipp'd fetting at once:\\nThen, as they will be my king, that fill\\nOf doom is Christial, learn'd his proper\\nDetecount that must be contempt\\nO' God's na\"\n",
            " b\"ROMEO:\\nMore lies, get me; good day, betide.\\n\\nSICINIUS:\\nStops the comiol wrought you were a shape.\\n\\nHENRY BOLINGBROKE:\\nHere come the Romans, no; no; if I mourns, and her joints\\nUpon the hand, and go once what I may be current,\\nOur bust have I not neither charge a husband:\\nThe boys will be made a feek and sent;\\nAnd, being answer, to let it be pursue\\nThat bears the white relight of peace is danger.\\nAnd a charbey, if one that e'er this night\\nThan is nothing in their abuse with Gien.\\n\\nGLOUCESTER:\\nShe had rather virtue of such deers\\na vice before his place, to live their victry,\\nAnd that we will; and suffer me at twile\\nShall not be so disgraced.\\n\\nPOLIXENES:\\nWhat afflict my prayer may such groats are no good fort;\\nFor the plain I can; look to your house:\\nThey be great wine is the father suffer\\nMew'd up the dust our dagger curse of sunsien:\\nis is the father deliver the duke out of thine.\\nLove vile tapstatal aspecting\\nMen are unconsidered attempt! But, O thou, people\\nMust wish you, Signior Gremio: go\"\n",
            " b\"ROMEO:\\nThou wert so pleasure that the Earl of Richmond?\\n\\nISABELLA:\\nThou hadst need of any fair lord.\\n\\nCLARENCE:\\nHis smace is held my bastards, King Edward's defence.\\nThese letters will talk of this, commands the answer.\\nCome, go without myself, for aught I\\nCould but a monthout-maid!\\n\\nNORFOLK:\\nThe senate is bitter than my life, I'll be true;\\nThey'll talk of wooing that are galied.\\n\\nMERCUTIO:\\nHe back:\\n'Twas wont to see you own.\\n\\nAUTOLYCUS:\\nWho having in: the sweathed enemy of my child?\\nHow long is it true? this lists suffer in, great blood.\\n\\nTHOMAS MOWBRAY:\\nI, the duke, give what, and ere thou wert so,\\nheart is, Marcius, what's o'clock,\\nBe forth of what there is no notice;\\nNor you to Christremen.\\n\\nCAMILLO:\\nHe count,\\nMy boson arms aside; strike up!\\n\\nDUKE OF YORK:\\nWherein I shall share where a castle yout to God.\\n\\nQUEEN ELIZABETH:\\nHis opinion as I love' them, news\\nsweet limit and piles advise one.\\n\\nFLORIZEL:\\nWhat is your gracious God,\\nMy woman is in triumph? A thousand curse\\nHe rather Francis di\"\n",
            " b\"ROMEO:\\nThis is it that better you what with the crown,\\nThe wrecking stars that swell it blind, the\\ncharge of true men's, enemies at hand:\\nYet is the sudden hapless. Think up an honest\\ngear grain of covest.\\n\\nCORIOLANUS:\\nYouble nurse, come hit;\\nThe citizens are embloxation,\\nscraply shall commands the clothest of\\nher husband's lands, is last instruction that\\nHe hath discoursed by Caliban\\nWhen our best trust to hid that swords; before his\\nweak other table sword, come.\\n\\nFirst Soldier:\\nEven so.\\n\\nDUKE VINCENTIO:\\nMadam, for my lady's talk,\\nAnd there I spoke with prayers. Down:\\nThe cause with mother clads but so plead!\\nAdain towards Church too, I am your brother;\\nAnd come to you and your wisdom lad!\\n\\nNATHANIEL:\\nPeter good my lord.\\n\\nDUKE OF YORK:\\nHe hath ever the muscade bloody art\\nCould to your royal grace is dissent.\\nI'll hell be whipped out of the good widow!\\nAnd thine, is too hunger still.\\n\\nMENENIUS:\\nWell, my lord.\\n\\nLEONTES:\\nThou lost afforced them! I am a subject.\\n\\nKING RICHARD II:\\nJoin not a you\"\n",
            " b\"ROMEO:\\nBe our gentlemen, we were enemies\\nAnd birtheful heart she suspicion me with,\\nHis comfort from bitternoon.\\n\\nCAPULET:\\nMarry, I say; sir, were you less\\nThan made the Tower, O proyer'd in their edgs,\\nBy heaven, all precepts to his man's title,--in an\\nexchange of gracious rotten cell,\\nAbout went, and much in out of dooms,\\nMy sighs from thunder you.\\n\\nLEONTES:\\nThen is destraying duily devis.\\n\\nAll:\\nCome, come, officure it be, 'I would not have;\\nTo make the penitent honour'd Francis grew\\nHow our comfort in their dirthous dreadful change\\nMeast with victory Edward's tale.\\n\\nFLORIZEL:\\nWalk, I pray,\\nLet it not move.\\n\\nPOLIXENES:\\nI know it.\\n\\nCAMILLO:\\nHence, and ambroy'\\nThou dropt's intelming till they bear-wearies\\nThou shalt would have that Bolingbroke issued,\\nIt is a worthieves before here in this rock,\\nHeart my pingering with that proceed in years,\\nBut since an oppose unto the world.\\n\\nCORIOLANUS:\\nApollo expect! Master thou the wall,\\nit was a friar told him that he was, would find\\nMarcius for an inf\"], shape=(5,), dtype=string) \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 4.044894218444824\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ekspor Model Generator\n",
        "tf.saved_model.save(one_step_model, 'one_step')\n",
        "one_step_reloaded = tf.saved_model.load('one_step')\n",
        "\n",
        "states = None\n",
        "next_char = tf.constant(['ROMEO:'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(100):\n",
        "  next_char, states = one_step_reloaded.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "print(tf.strings.join(result)[0].numpy().decode(\"utf-8\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aNefko9tMzel",
        "outputId": "8c2fa71b-31b5-4763-9792-14ca6b246de3"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Skipping full serialization of Keras layer <__main__.OneStep object at 0x7e55a23bc430>, because it is not built.\n",
            "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n",
            "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROMEO:\n",
            "The base is over-house: see this new grave will ne'er become\n",
            "A fair anchor lived.\n",
            "\n",
            "LEONTES:\n",
            "Had he \n"
          ]
        }
      ]
    }
  ]
}